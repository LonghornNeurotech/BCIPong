{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset as TData\n",
    "from torch.utils.data import DataLoader as DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(signal, crit_freq = [1, 40], sampling_freq = 128, plot = False, channel = 0):\n",
    "  order = 4\n",
    "\n",
    "  b, a = scipy.signal.butter(2, crit_freq, btype = 'bandpass', fs = sampling_freq)\n",
    "  processed_signal = scipy.signal.filtfilt(b, a, signal, 1)\n",
    "\n",
    "  if plot == True:\n",
    "    plt.figure()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'Normalized amplitude of channel {channel}')\n",
    "    plt.title(f'{crit_freq[0]}-{crit_freq[1]}Hz bandpass filter')\n",
    "    signal_min = np.full((signal.shape[1], signal.shape[0]), np.min(signal, 1)).transpose()\n",
    "    signal_max = np.full((signal.shape[1], signal.shape[0]), np.max(signal, 1)).transpose()\n",
    "    normed_signal = (signal - signal_min) / (signal_max - signal_min)\n",
    "    filtered_min = np.full((processed_signal.shape[1], processed_signal.shape[0]), np.min(processed_signal, 1)).transpose()\n",
    "    filtered_max = np.full((processed_signal.shape[1], processed_signal.shape[0]), np.max(processed_signal, 1)).transpose()\n",
    "    normed_filt = (processed_signal - filtered_min) / (filtered_max - filtered_min)\n",
    "    plt.plot(np.arange(normed_signal[channel].size), normed_signal[channel], label = 'Input')\n",
    "    plt.plot(np.arange(normed_filt[channel].size), normed_filt[channel], label = 'Transformed')\n",
    "    plt.legend()\n",
    "\n",
    "  return processed_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(signal, sampling_freq=128, window_size=1, window_shift=0.016):\n",
    "  w_size = int(sampling_freq * window_size)\n",
    "  w_shift = int(sampling_freq * window_shift)\n",
    "  segments = []\n",
    "  i = 0\n",
    "  while i + w_size <= signal.shape[1]:\n",
    "    segments.append(signal[:, i: i + w_size])\n",
    "    i += w_shift\n",
    "  return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_nathan_LR'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m new_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_nathan_LR\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(new_base, file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[1;32m      5\u001b[0m file_splits \u001b[38;5;241m=\u001b[39m [file\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_nathan_LR'"
     ]
    }
   ],
   "source": [
    "new_base = \"data_nathan_LR\"\n",
    "files = os.listdir(new_base)\n",
    "paths = [os.path.join(new_base, file) for file in files]\n",
    "\n",
    "file_splits = [file.rstrip('.npy').split('_')[3] for file in files]\n",
    "sessions = map(int, set(file_splits))\n",
    "sorted_sessions = sorted(list(sessions))[8:] \n",
    "sorted_sessions.remove(31)\n",
    "sorted_sessions.remove(34)\n",
    "sorted_sessions.remove(41)\n",
    "sorted_sessions.remove(14)\n",
    "sorted_sessions.remove(18)\n",
    "sorted_sessions.remove(11)\n",
    "sorted_sessions.remove(28)\n",
    "sorted_sessions.remove(26)\n",
    "sorted_sessions.remove(32)\n",
    "sorted_sessions.remove(21)\n",
    "print(sorted_sessions) # 17 sessions in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_rearrangment(sig, channel_order):\n",
    "    channel_order = [channel - 1 for channel in channel_order]\n",
    "    reindexed = np.zeros_like(sig)\n",
    "    for i, ind in enumerate(channel_order):\n",
    "        reindexed[i] = sig[ind]\n",
    "    return reindexed\n",
    "\n",
    "ordered_channels = [1, 9, 11, 3, 13, 5, 7, 15, 2, 10, 12, 4, 14, 6, 8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class LimitedDataLoader:\n",
    "    def __init__(self, dataloader, max_batches):\n",
    "        self.dataloader = dataloader\n",
    "        self.max_batches = max_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        return itertools.islice(self.dataloader, self.max_batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataloader), self.max_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_model(lin, train_dl, valid_dl, index):\n",
    "  print(f'Training model {index}')\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.SGD(lin.parameters(), lr = 0.01, weight_decay=5e-3)\n",
    "  epochs = 25\n",
    "\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  accs = []\n",
    "  max_acc = 0\n",
    "  best_loss = 1000\n",
    "  for i in range(epochs):\n",
    "    total_train_loss = 0.0\n",
    "    lin.train()\n",
    "    pbar = tqdm(total=len(train_dl))\n",
    "    for j, (sig, labels) in tqdm(enumerate(train_dl)):\n",
    "      sig = sig.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      pred = lin(sig)\n",
    "      loss = criterion(pred, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      total_train_loss += loss.item()\n",
    "\n",
    "      pbar.set_description(f\"Epoch {i + 1}    loss={total_train_loss / (j + 1):0.4f}\")\n",
    "      pbar.update(1)\n",
    "      # if best loss is less than current loss, decrease learning rate\n",
    "    if  total_train_loss / len(train_dl) > best_loss:\n",
    "      for g in optimizer.param_groups:\n",
    "        g['lr'] *= 0.9\n",
    "    else:\n",
    "      best_loss = total_train_loss / len(train_dl)\n",
    "    pbar.close()\n",
    "    train_losses.append(total_train_loss / len(train_dl))\n",
    "    total_val_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    #count_val = 0\n",
    "    lin.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=len(valid_dl))\n",
    "        for j, (sig, labels) in enumerate(valid_dl):\n",
    "            # w_size = 128\n",
    "            # ind = 0\n",
    "            # hn = None\n",
    "            # while ind + w_size < sig.shape[1]:\n",
    "            #   signal = sig[:, :, ind: ind + w_size].to(device)\n",
    "            #   #signal = signal.transpose(1, 2)\n",
    "            #   labels = labels.to(device)\n",
    "              \n",
    "            #   pred = lin(signal)\n",
    "              \n",
    "            #   #pred = pred.reshape(-1, *pred.shape[2:]) \n",
    "            #   # pred = torch.argmax(pred, 2)\n",
    "            #   # pred = torch.mode(pred, 1)[0]\n",
    "\n",
    "            #   ind += 3\n",
    "              #count_val += 1\n",
    "            sig = sig.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = lin(sig)\n",
    "            loss = criterion(pred, labels)\n",
    "\n",
    "            prob_pred = torch.nn.functional.softmax(pred, -1)\n",
    "            acc = (prob_pred.argmax(-1) == labels.argmax(-1)).float().mean()\n",
    "            total_val_loss += loss.item()\n",
    "            total_accuracy += acc.item()\n",
    "\n",
    "            # if (i + 1) % 5 == 0 and j == (len(valid_dl) - 1):\n",
    "            #   print(f'val loss: {total_val_loss / (j + 1):.4f}    val accuracy: {total_accuracy / (j + 1):.4f}')\n",
    "            pbar.set_description(f\"val loss={total_val_loss / (j + 1):.4f}    val acc={total_accuracy / (j + 1):.4f}\")\n",
    "            pbar.update(1)\n",
    "        print()\n",
    "        pbar.close()\n",
    "        val_losses.append(total_val_loss / len(valid_dl))\n",
    "        accs.append(total_accuracy / len(valid_dl))\n",
    "        \n",
    "    # save model if val accuracy is less than val accuracy of previous epoch\n",
    "    if accs[-1] > max_acc:\n",
    "      ckpt = {'state_dict': lin.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "      torch.save(ckpt, 'parallelConvLSTM2.pth.tar')\n",
    "      print('model saved')\n",
    "      max_acc = accs[-1]\n",
    "\n",
    "  return {'index': index, 'train loss': train_losses, 'val_losses': val_losses, 'val accs': accs} # fuck you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # convolutional block for spatial feature extraction\n",
    "        self.block1 = torch.nn.Sequential(torch.nn.Conv1d(in_channels, 16, 64, padding = 'same', bias=False),\n",
    "                                           torch.nn.BatchNorm1d(16), \n",
    "                                           torch.nn.ReLU(),\n",
    "                                           torch.nn.Dropout(),\n",
    "                                           torch.nn.AvgPool1d(2))\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        # separable convolutional block for additional feature extraction\n",
    "        self.block2 = torch.nn.Sequential(torch.nn.Conv1d(16, 16, 16, padding = 'same', groups = 16, bias = False),\n",
    "                                           torch.nn.Conv1d(16, 24, 1, padding = 'same', bias = False),\n",
    "                                           torch.nn.BatchNorm1d(24),\n",
    "                                           torch.nn.ReLU(),\n",
    "                                           torch.nn.Dropout(),\n",
    "                                           torch.nn.AvgPool1d(4))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        y1 = self.flatten(out)\n",
    "        y2 = self.flatten(self.block2(out))\n",
    "        return y1, y2\n",
    "\n",
    "class OrderedSpatialConv(torch.nn.Module):\n",
    "    def __init__(self, groups, in_channels):\n",
    "        super().__init__()\n",
    "        self.groups = groups\n",
    "        self.channels = in_channels\n",
    "        self.block1 = torch.nn.Sequential(torch.nn.Conv2d(groups, 16, (3, 64), padding='same', groups = self.groups, bias = False), # return to 3, 16\n",
    "                                          torch.nn.BatchNorm2d(16), \n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.Dropout(),\n",
    "                                          torch.nn.AvgPool2d((1, 2))) # return to 2\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.block2 = torch.nn.Sequential(torch.nn.Conv2d(16, 16, (3, 16), padding = 'same', groups = 16, bias = False), # return to 3, 16\n",
    "                                           torch.nn.Conv2d(16, 24, 1, padding = 'same', bias = False),\n",
    "                                           torch.nn.BatchNorm2d(24),\n",
    "                                           torch.nn.ReLU(),\n",
    "                                           torch.nn.Dropout(),\n",
    "                                           torch.nn.AvgPool2d((2, 4)))\n",
    "                                        #    torch.nn.Conv2d(24, 24, (2, 1)),\n",
    "                                        #    torch.nn.ReLU(),\n",
    "                                        #    torch.nn.Dropout())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, self.groups, int(self.channels / self.groups), -1)\n",
    "        out = self.block1(x)\n",
    "        y1 = self.flatten(out)\n",
    "        y2 = self.flatten(self.block2(out))\n",
    "        return y1, y2\n",
    "        \n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=50, n_cells=3):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_channels, hidden_channels, n_cells, batch_first=True, dropout = 0.4)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "    def forward(self, x, h0 = None):\n",
    "        y, _ = self.lstm(x, h0)\n",
    "        y = self.flatten(y)\n",
    "        return y\n",
    "\n",
    "class FusionLinear(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.spatial_fe = OrderedSpatialConv(4, in_channels)\n",
    "        self.temporal_fe = LSTM(in_channels)\n",
    "\n",
    "        # pass dummy input through spatial and temporal feature extractors to determine output size\n",
    "        dummy_x = torch.zeros(1, in_channels, 192)\n",
    "        out1 = self.spatial_fe(dummy_x)\n",
    "        #print(out1[0].shape)\n",
    "        #print(out1[1].shape)\n",
    "        out2 = self.temporal_fe(dummy_x.transpose(1, 2))\n",
    "        #print(out2.shape)\n",
    "        dummy_out = torch.hstack([*out1, out2])\n",
    "        self.l1 = torch.nn.Linear(dummy_out.shape[1], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spatial_x = self.spatial_fe(x)\n",
    "        temporal_x = self.temporal_fe(x.transpose(1, 2))\n",
    "        linear_input = torch.hstack([*spatial_x, temporal_x])\n",
    "        y = self.l1(linear_input)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(signals, labels, n):\n",
    "    # Convert signals and labels to numpy arrays once\n",
    "    signals, labels = np.array(signals), np.array(labels)\n",
    "    \n",
    "    # Find the signals with specific labels\n",
    "    signals_left, signals_right = signals[np.where(labels == 1)], signals[np.where(labels == 3)]\n",
    "    \n",
    "    # Initialize the arrays for augmented data\n",
    "\n",
    "    augmented, augmented_labels = np.empty((n, signals.shape[1], signals.shape[2]), dtype=signals.dtype), np.empty(n, dtype=labels.dtype)\n",
    "    \n",
    "    # Pre-generate indices for random choices\n",
    "    left_indices, right_indices, random_uniforms, signal_indices = np.random.choice(len(signals_left), n), np.random.choice(len(signals_right), n), np.random.rand(n), np.random.choice(len(signals), n)\n",
    "\n",
    "    for i in range(n):\n",
    "        sig1_index = signal_indices[i]\n",
    "        sig1 = signals[sig1_index]\n",
    "        \n",
    "        if sig1_index in np.where(labels == 1)[0]:\n",
    "            sig2 = signals_right[right_indices[i]]\n",
    "            augmented_labels[i] = 1\n",
    "        else:\n",
    "            sig2 = signals_left[left_indices[i]]\n",
    "            augmented_labels[i] = 3\n",
    "        \n",
    "        index = int(random_uniforms[i] * sig1.shape[1])\n",
    "        \n",
    "        # Use pre-allocated array for augmented signal\n",
    "        augmented[i, :, :index] = sig1[:, :index]\n",
    "        augmented[i, :, index:] = sig2[:, index:]\n",
    "\n",
    "\n",
    "    del signals_left\n",
    "    del signals_right\n",
    "    del random_uniforms\n",
    "    del signal_indices\n",
    "    \n",
    "    return augmented, augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "cimport numpy as np\n",
    "from libc.math cimport exp\n",
    "\n",
    "def swap_components(list dec1, list dec2, np.ndarray[np.int32_t, ndim=1] swap_flags):\n",
    "    cdef int j\n",
    "    cdef np.ndarray tmp\n",
    "    for j in range(1, 4):\n",
    "        if swap_flags[j-1]:\n",
    "            tmp = dec1[j]\n",
    "            dec1[j] = dec2[j]\n",
    "            dec2[j] = tmp\n",
    "    return dec1, dec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "\n",
    "def wavelet_augmentation(signals, labels, n):\n",
    "    signals, labels = np.array(signals), np.array(labels)\n",
    "    signals_left, signals_right = signals[labels == 1], signals[labels == 3]\n",
    "    augmented = np.empty((n, signals.shape[1], signals.shape[2]), dtype=signals.dtype)\n",
    "    augmented_labels = np.empty(n, dtype=labels.dtype)\n",
    "\n",
    "    left_indices = np.random.choice(len(signals_left), n)\n",
    "    right_indices = np.random.choice(len(signals_right), n)\n",
    "\n",
    "    # Adjust the number of augmentations per iteration\n",
    "    n_half = n // 2\n",
    "\n",
    "    # Pre-generate random choices and random swaps\n",
    "    signal_indices = np.random.choice(len(signals), n_half)\n",
    "    swap_flags = np.random.rand(n_half, 3) < 0.5\n",
    "\n",
    "    left_membership = np.isin(np.arange(len(signals)), np.where(labels == 1)) # calculating this reduces time by 130%\n",
    "\n",
    "    for i in range(n_half):\n",
    "        sig1_index = signal_indices[i]\n",
    "        sig1 = signals[sig1_index]\n",
    "        if left_membership[sig1_index]:\n",
    "            sig2 = signals_right[right_indices[i]]\n",
    "            augmented_labels[i*2] = 1\n",
    "            augmented_labels[i*2 + 1] = 3\n",
    "        else:\n",
    "            sig2 = signals_left[left_indices[i]]\n",
    "            augmented_labels[i*2] = 3\n",
    "            augmented_labels[i*2 + 1] = 1\n",
    "        dec1 = pywt.wavedec(sig1, 'db4', level=3)\n",
    "        dec2 = pywt.wavedec(sig2, 'db4', level=3)\n",
    "\n",
    "        # much slower in pure python/numpy\n",
    "        dec1, dec2 = swap_components(dec1, dec2, swap_flags[i].astype(np.int32))\n",
    "\n",
    "        augmented[i*2] = pywt.waverec(dec1, 'db4')\n",
    "        augmented[i*2 + 1] = pywt.waverec(dec2, 'db4')\n",
    "\n",
    "    return augmented, augmented_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientCapsnetPytorch.model.losses import TotalLoss, MarginLoss\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_caps(train_loader, test_loader, index):\n",
    "    model = EEGCapsNet(input_size=(1, 16, 192), num_classes=2)\n",
    "    model.cuda()\n",
    "    criterion = TotalLoss(recon_factor=0.0005)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    deep_super_weights = [1/(1.5**i) for i in range(4)]\n",
    "    deep_super_weights = deep_super_weights / np.sum(deep_super_weights)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    accs = []\n",
    "    max_acc = 0\n",
    "    best_loss = 1000\n",
    "    \n",
    "    for i in range(35):\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(total=len(train_loader))\n",
    "        model.train()\n",
    "        for j, (x, y) in enumerate(train_loader):\n",
    "            x = x.float().cuda()\n",
    "            y = y.cuda()\n",
    "            x = x.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outs = model(x)\n",
    "            y = y.squeeze(1)\n",
    "            full_loss = 0\n",
    "            x_fft = torch.fft.fft(x, dim=-1)\n",
    "            for k in range(0, 8, 2):\n",
    "                loss = criterion(x_fft, y, outs[k+1], outs[k])\n",
    "                loss *= deep_super_weights[k // 2]\n",
    "                full_loss += loss\n",
    "                \n",
    "            full_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += full_loss.item()\n",
    "            pbar.set_description(f\"Epoch {i + 1}    loss={total_loss / (j + 1):0.4f}\")\n",
    "            pbar.update(1)\n",
    "        \n",
    "        if  total_loss / len(train_loader) > best_loss:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] *= 0.9\n",
    "            else:\n",
    "                best_loss = total_loss / len(train_loader)\n",
    "        pbar.close()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        test_loss = 0\n",
    "        total_accuracy = 0\n",
    "        model.eval()\n",
    "            \n",
    "        test_loss = 0\n",
    "        total_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(total=len(test_loader))\n",
    "            for j, (x, y) in enumerate(test_loader):\n",
    "                x = x.float().cuda()\n",
    "                y = y.cuda()\n",
    "                x = x.unsqueeze(1)\n",
    "                outs = model(x, mode='eval')\n",
    "                pred = outs[0]\n",
    "                img = outs[1]\n",
    "                y = y.squeeze(1)\n",
    "                x_fft = torch.fft.fft(x, dim=-1)\n",
    "                loss = criterion(x_fft, y, img, pred)\n",
    "                test_loss += loss.item()\n",
    "                predicted = torch.argmax(pred, -1)\n",
    "                labels = torch.argmax(y, -1)\n",
    "                accuracy = (predicted == labels).float().mean().item()\n",
    "                total_accuracy += accuracy\n",
    "                pbar.set_description(f\"val loss={test_loss / (j + 1):0.4f}    val acc={total_accuracy / (j + 1):0.4f}\")\n",
    "                pbar.update(1)\n",
    "            if total_accuracy / len(test_loader) > max_acc:\n",
    "                ckpt = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "                torch.save(ckpt, f'capsnet_{index}.pth')\n",
    "                print('model saved')\n",
    "                max_acc = total_accuracy / len(test_loader)\n",
    "            print()\n",
    "            pbar.close()\n",
    "            val_losses.append(test_loss / len(test_loader))\n",
    "            accs.append(total_accuracy / len(test_loader))\n",
    "            x = torch.abs(torch.fft.ifft(x, dim=-1))\n",
    "            img = torch.abs(torch.fft.ifft(img, dim=-1))\n",
    "            plt.plot(x[0, 0, 3, :100].cpu().numpy(), label='input')\n",
    "            plt.plot(img[0, 0, 3, :100].cpu().numpy(), label='reconstruction')\n",
    "            # mu and beta bands\n",
    "            plt.axvline(8, color='r')\n",
    "            plt.axvline(30, color='r')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "    return {'index': index, 'train loss': train_losses, 'val_losses': val_losses, 'val accs': accs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Squash(nn.Module):\n",
    "    def __init__(self, eps=1e-20):\n",
    "        super(Squash, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.linalg.norm(x, ord=2, dim=-1, keepdim=True)\n",
    "        coef = 1 - 1 / (torch.exp(norm) + self.eps)\n",
    "        unit = x / (norm + self.eps)\n",
    "        return coef * unit\n",
    "\n",
    "class Routing(nn.Module):\n",
    "    def __init__(self, groups, in_dims, out_dims):\n",
    "        super(Routing, self).__init__()\n",
    "        N0, D0 = in_dims\n",
    "        N1, self.D1 = out_dims\n",
    "        self.W = nn.Parameter(torch.Tensor(groups, N1, N0, D0, self.D1))\n",
    "        nn.init.kaiming_normal_(self.W)\n",
    "        self.b = nn.Parameter(torch.zeros(groups, N1, N0, 1))\n",
    "        self.squash = Squash()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        u = torch.einsum('...gni,gknid->...gknd', x, self.W) # shape: (B, G, N1, N0, D1)\n",
    "\n",
    "        c = torch.einsum(\"...ij,...kj->...i\", u, u) # shape: (B, N1, N0)\n",
    "\n",
    "        c = c[..., None]  # (B, N1, N0, 1) for bias broadcasting\n",
    "        c = c / torch.sqrt(torch.tensor(self.D1).float())  # stabilize\n",
    "        c = torch.softmax(c, axis=1) + self.b\n",
    "\n",
    "        ## new capsules\n",
    "        s = torch.sum(u * c, dim=-2)\n",
    "\n",
    "        return self.squash(s)\n",
    "    \n",
    "\n",
    "class ReconstructionNet(nn.Module):\n",
    "    def __init__(self, input_size=(1, 28, 28), num_classes=2, num_capsules=64):\n",
    "        super(ReconstructionNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(in_features=num_capsules * num_classes, out_features=512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, np.prod(input_size) * 2)\n",
    "        self.elu = nn.ReLU()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.fc1.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc2.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc3.weight, gain=gain)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.fc1(x))\n",
    "        x = self.elu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(x.size(0), 2, *self.input_size).squeeze(1)\n",
    "        complex_x = torch.complex(x[:, 0], x[:, 1])\n",
    "        return complex_x\n",
    "    \n",
    "\n",
    "class CapsMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsMask, self).__init__()\n",
    "\n",
    "    def forward(self, x, y_true=None):\n",
    "        if y_true is not None:  # training mode\n",
    "            mask = y_true\n",
    "        else:  # testing mode\n",
    "            # convert list of maximum value's indices to one-hot tensor\n",
    "            temp = torch.sqrt(torch.sum(x**2, dim=-1))\n",
    "            mask = F.one_hot(torch.argmax(temp, dim=1), num_classes=temp.shape[1])\n",
    "        \n",
    "        masked = x * mask.unsqueeze(-1)\n",
    "\n",
    "        return masked.view(x.shape[0], -1)  # reshape\n",
    "    \n",
    "\n",
    "class CapsLen(nn.Module):\n",
    "    def __init__(self, eps=1e-7):\n",
    "        super(CapsLen, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sqrt(\n",
    "            torch.sum(x**2, dim=-1) + self.eps\n",
    "        )  # (batch_size, num_capsules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EEGCapsNet(nn.Module):\n",
    "    def __init__(self, input_size=(1, 16, 192), num_classes=2):\n",
    "        super(EEGCapsNet, self).__init__()\n",
    "        self.channelCapsTemporal_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=128, kernel_size=(1,64), groups=16, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(1,input_size[2]-63), groups=128) # collapse to 1 point\n",
    "        )\n",
    "        self.channelCapsTemporal_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=128, kernel_size=(1, 24), groups=16, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(1,input_size[2]-23), groups=128) # collapse to 1 point\n",
    "        )\n",
    "        # channel caps will eventually be transformed from 8x4 capsules to 4x8 higher level capsules\n",
    "\n",
    "        self.channelRouting = Routing(16, (8, 4), (4, 8))\n",
    "        self.channelShrink = Routing(8, (8, 8), (4, 8))\n",
    "        self.channelDeepSuper = Routing(1, (64, 8), (2, 32))\n",
    "\n",
    "        self.localCapsSpatial_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=64, kernel_size=(2, 36), groups=8, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1, input_size[2]-35), groups=64) # collapse to 1 point\n",
    "        )\n",
    "        \n",
    "        self.localCapsSpatial_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=64, kernel_size=(2, 16), groups=8),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1, input_size[2]-15), groups=64) # collapse to 1 point\n",
    "        )\n",
    "\n",
    "        self.localRouting = Routing(8, (8, 4), (4, 8))\n",
    "        self.localShrink = Routing(8, (8, 8), (4, 16))\n",
    "        self.localRegion = Routing(4, (8, 16), (4, 16))\n",
    "        self.localDeepSuper = Routing(1, (32, 8), (2, 64))\n",
    "\n",
    "        # local spatial caps will be transformed from 8x4 capsules to 8x8 higher level capsules\n",
    "        # local caps will be 16x8 (with addition from channel caps) this will be reduced to 8x8\n",
    "\n",
    "\n",
    "        self.regionCapsSpatial_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=(4, 24), groups=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1, input_size[2]-23), groups=32) # collapse to 1 point\n",
    "        )\n",
    "\n",
    "        self.regionCapsSpatial_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=(4, 32), groups=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1, input_size[2]-31), groups=32) # collapse to 1 point\n",
    "        )\n",
    "\n",
    "        # region caps will be transformed from 8x4 capsules to 16x8 higher level capsules\n",
    "        # region caps will be 32x8 (with addition from local caps) this will be reduced to 16x8\n",
    "\n",
    "        self.regionRouting = Routing(4, (8, 4), (4, 16))\n",
    "        self.regionShrink = Routing(4, (8, 16), (4, 16))\n",
    "        self.regionHemi = Routing(2, (8, 16), (4, 16))\n",
    "        self.regionDeepSuper = Routing(1, (16, 16), (2, 64))\n",
    "\n",
    "\n",
    "        self.hemiCapsSpatial_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=16, kernel_size=(8, 30), groups=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(1, input_size[2]-29), groups=16) # collapse to 1 point\n",
    "        )\n",
    "\n",
    "        self.hemiCapsSpatial_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=16, kernel_size=(8, 60), groups=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(1, input_size[2]-59), groups=16) # collapse to 1 point\n",
    "        )\n",
    "\n",
    "        # hemi caps will be transformed from 8x4 capsules to 32x8 higher level capsules\n",
    "        # hemi caps will be 64x8 (with addition from region caps) this will be reduced to 32x8\n",
    "\n",
    "        self.hemiRouting = Routing(2, (8, 4), (4, 16))\n",
    "        self.hemiShrink = Routing(2, (8, 16), (4, 16))\n",
    "\n",
    "        self.out = Routing(1, (8, 16), (num_classes, 64))\n",
    "        self.generator = ReconstructionNet(input_size, num_classes)\n",
    "        self.channel_generator = ReconstructionNet(input_size, num_classes, num_capsules=32)\n",
    "        self.local_generator = ReconstructionNet(input_size, num_classes, num_capsules=64)\n",
    "        self.region_generator = ReconstructionNet(input_size, num_classes, num_capsules=64)\n",
    "        self.mask = CapsMask()\n",
    "        self.capsLen = CapsLen()\n",
    "\n",
    "\n",
    "    def forward(self, x, y_true=None, mode='train'):\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        data = x\n",
    "        x1 = self.channelCapsTemporal_1(data).view(data.size(0), 16, 16)\n",
    "        x2 = self.channelCapsTemporal_2(data).view(data.size(0), 16, 16)\n",
    "        channels = torch.cat((x1, x2), dim=2).view(data.size(0), 16, 8, 4)\n",
    "        channels = self.channelRouting(channels)\n",
    "        deep_channels = self.channelDeepSuper(channels.view(data.size(0), 1, 64, 8))\n",
    "        new_locals = self.channelShrink(channels.view(data.size(0), 8, 8, 8))\n",
    "\n",
    "        x1 = self.localCapsSpatial_1(data.view(data.size(0), 8, 2, 192)).view(data.size(0), 8, 16)\n",
    "        x2 = self.localCapsSpatial_2(data.view(data.size(0), 8, 2, 192)).view(data.size(0), 8, 16)\n",
    "        local = torch.cat((x1, x2), dim=2).view(data.size(0), 8, 8, 4)\n",
    "        local = self.localRouting(local)\n",
    "        deep_locals = self.localDeepSuper(local.view(data.size(0), 1, 32, 8))\n",
    "\n",
    "        local = torch.cat((local, new_locals), dim=2)\n",
    "        local = self.localShrink(local)\n",
    "        new_regions = self.localRegion(local.view(data.size(0), 4, 8, 16))\n",
    "\n",
    "        x1 = self.regionCapsSpatial_1(data.view(data.size(0), 4, 4, 192)).view(data.size(0), 4, 16)\n",
    "        x2 = self.regionCapsSpatial_2(data.view(data.size(0), 4, 4, 192)).view(data.size(0), 4, 16)\n",
    "        regions = torch.cat((x1, x2), dim=2).view(data.size(0), 4, 8, 4)\n",
    "        regions = self.regionRouting(regions)\n",
    "        deep_regions = self.regionDeepSuper(regions.view(data.size(0), 1, 16, 16))\n",
    "        regions = torch.cat((regions, new_regions), dim=2)\n",
    "        regions = self.regionShrink(regions)\n",
    "        new_hemis = self.regionHemi(regions.view(data.size(0), 2, 8, 16))\n",
    "\n",
    "        x1 = self.hemiCapsSpatial_1(data.view(data.size(0), 2, 8, 192)).view(data.size(0), 2, 16)\n",
    "        x2 = self.hemiCapsSpatial_2(data.view(data.size(0), 2, 8, 192)).view(data.size(0), 2, 16)\n",
    "        hemis = torch.cat((x1, x2), dim=2).view(data.size(0), 2, 8, 4)\n",
    "        hemis = self.hemiRouting(hemis)\n",
    "        hemis = torch.cat((hemis, new_hemis), dim=2)\n",
    "        hemis = self.hemiShrink(hemis).view(data.size(0), 1, 8, 16)\n",
    "\n",
    "        out = self.out(hemis)\n",
    "        out = out.squeeze(1)\n",
    "\n",
    "        pred = self.capsLen(out)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            masked = self.mask(out, y_true)\n",
    "            deep_channels = deep_channels.squeeze(1)\n",
    "            deep_locals = deep_locals.squeeze(1)\n",
    "            deep_regions = deep_regions.squeeze(1)\n",
    "            masked_channels = self.mask(deep_channels, y_true)\n",
    "            masked_locals = self.mask(deep_locals, y_true)\n",
    "            masked_regions = self.mask(deep_regions, y_true)\n",
    "        elif mode == \"eval\":\n",
    "            masked = self.mask(out)\n",
    "            x = self.generator(masked)\n",
    "            return pred, x\n",
    "        elif mode == \"test\":\n",
    "            return pred\n",
    "        x = self.generator(masked)\n",
    "        x_channels = self.channel_generator(masked_channels)\n",
    "        x_locals = self.local_generator(masked_locals)\n",
    "        x_regions = self.region_generator(masked_regions)\n",
    "        pred_channels = self.capsLen(deep_channels)\n",
    "        pred_locals = self.capsLen(deep_locals)\n",
    "        pred_regions = self.capsLen(deep_regions)\n",
    "\n",
    "   \n",
    "        return pred, x, pred_regions, x_regions, pred_locals, x_locals, pred_channels, x_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 9 10 12 13 15 16 17 19 20 22 23 24 25 27 29 30 33 35 36 37 38 39 40 42\n",
      " 43 44 45 46 47 48 49 50 51 53 54 55 56 57] TEST: [52]\n",
      "[52]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2507124/3986792437.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  filtered_sig = (filtered_sig - np.mean(filtered_sig, 1, keepdims=True)) / np.std(filtered_sig, 1, keepdims=True) # standard scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEGMouse_Nathan_1_47_1173.npy\n",
      "EEGMouse_Nathan_1_50_1240.npy\n",
      "EEGMouse_Nathan_1_51_1262.npy\n",
      "Creating augmented data\n"
     ]
    }
   ],
   "source": [
    "# do n-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "session_info = []\n",
    "sorted_sessions = np.array(sorted_sessions)\n",
    "# clear all tqdm bars\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=len(sorted_sessions), shuffle=True, random_state=42)\n",
    "test_sessions = []\n",
    "val_sessions = []\n",
    "train_sessions = []\n",
    "for train_index, test_index in kfold.split(sorted_sessions):\n",
    "  train_sessions, val_sessions = sorted_sessions[train_index], sorted_sessions[test_index]\n",
    "  print(\"TRAIN:\", train_sessions, \"TEST:\", val_sessions)\n",
    "\n",
    "  train_eeg = []\n",
    "  train_labels = []\n",
    "  valid_eeg = []\n",
    "  valid_labels = []\n",
    "  test_eeg = []\n",
    "  test_labels = []\n",
    "  print(val_sessions)\n",
    "  for i in range(len(files)):\n",
    "    name = files[i]\n",
    "    details = name.rstrip('.npy').split('_')[2:] # getting session details from file name\n",
    "    sig = np.load(paths[i]) # loading signal \n",
    "    sig = sig[:, 1:] # removing first time step because it is inaccurate\n",
    "    if sig.shape[1] == 0 or int(details[1]) not in sorted_sessions:\n",
    "      #print(name)\n",
    "      continue\n",
    "    reindexed_signal = channel_rearrangment(sig, ordered_channels)\n",
    "    filtered_sig = bandpass_filter(reindexed_signal, [5, 40], 125) # bandpass filter\n",
    "    filtered_sig = (filtered_sig - np.mean(filtered_sig, 1, keepdims=True)) / np.std(filtered_sig, 1, keepdims=True) # standard scaling\n",
    "    signals = segmentation(filtered_sig, 128, window_size = 1.5, window_shift = 0.0175) # segmentation\n",
    "    if np.isnan(signals).any():\n",
    "      print(name)\n",
    "      continue\n",
    "    labels = [int(details[0])] * len(signals)\n",
    "    if int(details[1]) in test_sessions:\n",
    "      print(\"this should not occur\")\n",
    "      test_eeg.extend(signals)\n",
    "      test_labels.extend(labels)\n",
    "    elif int(details[1]) in val_sessions:\n",
    "      valid_eeg.extend(signals)\n",
    "      valid_labels.extend(labels)\n",
    "    else:\n",
    "      train_eeg.extend(signals)\n",
    "      train_labels.extend(labels)\n",
    "    \n",
    "  print(\"Creating augmented data\")\n",
    "  aug_signals, aug_labels = augmentation(train_eeg, train_labels, 20000)\n",
    "  train_eeg.extend(aug_signals)\n",
    "  train_labels.extend(aug_labels)\n",
    "  '''aug_signals, aug_labels = wavelet_augmentation(train_eeg, train_labels, 10000)\n",
    "  train_eeg.extend(aug_signals)\n",
    "  train_labels.extend(aug_labels)'''\n",
    "  print(\"Data created\")\n",
    "\n",
    "\n",
    "  train_eeg_tensor = torch.zeros((len(train_eeg), train_eeg[0].shape[0], train_eeg[0].shape[1])) # untransposed dimensions 1 and 2\n",
    "  valid_eeg_tensor = torch.zeros((len(valid_eeg), valid_eeg[0].shape[0], valid_eeg[0].shape[1]))\n",
    "  #test_eeg_tensor = torch.zeros((len(test_eeg), test_eeg[0].shape[0], test_eeg[0].shape[1]))\n",
    "  for i in range(len(train_eeg)):\n",
    "    tens = torch.from_numpy(train_eeg[i].copy()) # no longer transposing before conversion to tensor\n",
    "    train_eeg_tensor[i] = tens\n",
    "  for i in range(len(valid_eeg)):\n",
    "    tens = torch.from_numpy(valid_eeg[i].copy())\n",
    "    valid_eeg_tensor[i] = tens\n",
    "  '''for i in range(len(test_eeg)):S\n",
    "    tens = torch.from_numpy(test_eeg[i].copy())\n",
    "    test_eeg_tensor[i] = tens'''\n",
    "  train_label_tensor = torch.zeros(len(train_labels), 2)\n",
    "  valid_label_tensor = torch.zeros(len(valid_labels), 2)\n",
    "  test_label_tensor = torch.zeros(len(test_labels), 2)\n",
    "  class_to_idx = {1:0, 3:1}\n",
    "  for i in range(len(train_labels)):\n",
    "    label = class_to_idx[train_labels[i]]\n",
    "    train_label_tensor[i][label] = 1\n",
    "  for i in range(len(valid_labels)):\n",
    "    label = class_to_idx[valid_labels[i]]\n",
    "    valid_label_tensor[i][label] = 1\n",
    "  for i in range(len(test_labels)):\n",
    "    label = class_to_idx[test_labels[i]]\n",
    "    test_label_tensor[i][label] = 1\n",
    "\n",
    "  train_ds = TData(train_eeg_tensor, train_label_tensor)\n",
    "  valid_ds = TData(valid_eeg_tensor, valid_label_tensor)\n",
    "  #test_ds = TData(test_eeg_tensor, test_label_tensor)\n",
    "  train_dl = DL(train_ds, batch_size=32, shuffle= True, drop_last = True, pin_memory=True)\n",
    "  valid_dl = DL(valid_ds, batch_size=32, shuffle= True, pin_memory=True)\n",
    "  #test_dl = DL(test_ds, batch_size=32, shuffle = True, drop_last = True)\n",
    "  train_dl = LimitedDataLoader(train_dl, 250)\n",
    "  device = 'cuda'\n",
    "  lin = FusionLinear(16, 2).to(device)\n",
    "  info = train_caps(train_dl, valid_dl, val_sessions[0])\n",
    "  session_info.append(info)\n",
    "  print()\n",
    "  print(\"=====================================================================================================\")\n",
    "  print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load in the json file \n",
    "with open('max_accs_new_aug.json') as f:\n",
    "    prev_accs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accs = []\n",
    "for i in range(len(session_info)):\n",
    "    print(f\"Session {session_info[i]['index']}    max accuracy: {max(session_info[i]['val accs']):.4f}\")\n",
    "    max_accs.append({'session': session_info[i]['index'], 'max_acc': max(session_info[i]['val accs'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average max accuracy\n",
    "avg_max = sum([x['max_acc'] for x in max_accs]) / len(max_accs)\n",
    "avg_max_aug = sum([prev_accs[key] for key in prev_accs]) / len(prev_accs)\n",
    "avg_max, avg_max_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by max_acc\n",
    "max_accs = sorted(max_accs, key = lambda x: x['max_acc'], reverse = True)\n",
    "max_accs_dict = {}\n",
    "for i in range(len(max_accs)):\n",
    "    max_accs_dict[int(max_accs[i]['session'][0])] = max_accs[i]['max_acc']\n",
    "    print(f\"Session {max_accs[i]['session']}    max accuracy: {max_accs[i]['max_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(max_accs_dict.keys(), max_accs_dict.values(), alpha = 0.5)\n",
    "plt.bar(max_accs_dict.keys(), [prev_accs[str(key)] for key in max_accs_dict.keys() if str(key) in prev_accs], alpha = 0.5)\n",
    "plt.xlabel('Session')\n",
    "plt.ylabel('Max accuracy')\n",
    "plt.title('Max accuracy of each session')\n",
    "plt.legend(['w/ augmentation', 'w/o augmentation'])\n",
    "plt.show()\n",
    "# print average accuracy of new model\n",
    "print(f\"Average accuracy w/ augmentation: {np.mean(list(max_accs_dict.values())):.4f}\")\n",
    "# print average accuracy of old model given the sessions in the new model\n",
    "print(f\"Average accuracy w/o augmentation: {np.mean(list(prev_accs.values())):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('max_accs_new_aug_3.json', 'w') as f:\n",
    "    json.dump(max_accs_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swinunetr2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
